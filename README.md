# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
The dataset contains data about the personal and financial details of the customers of a Portugese bank. We seek to predict if the customer will subscribe to bank term deposit.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best performing model was a Soft Voting Ensemble found using AutoML. It contained XGBoost Classifier with a standard scaler wrapper.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The dataset is first retrieved using AzureDataFactory class.
Some preprocessing steps were performed like converting categorical variable to binary encoding, one hot encoding
Then dataset is split in ratio of 67/33 (train/test)
Then the model is trained using the hyperparameters passed from command line
The hyperparameters that can be tuned are C an max_iter. C is the inverse regularization parameter and max_iter is the maximum number of iterations.
Then accuracy was calculated on the test set which is also the defining metric.

**What are the benefits of the parameter sampler you chose?**
The inverse parameter value C is one of the important parameters from business context and it controls the overfitting of the model. Normally a good thing is to see the change in accuracy by increasing or decreasing C by 10x
The number of iterations is also an important factor because if the problem is too complex then they do help in training for longer durations.

**What are the benefits of the early stopping policy you chose?**
It saves a lot of computational resources
It saves a lot of time for the Data Scientist as we have clearly defined the patience level and the deviation that we can expect at maximum.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
AutoML can do choose best performing models and the best performing hyper parameters for those models. The models were XGBoost, LightGBM, RandomForests, BoostedTrees, SGDClassifier with varying input preprocessing normalizations like: Min Max Scaling, Standard Scaling etc.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?

The difference in accuracy is not too much. AutoML accuracy -> 0.9175 Hyperdrive accuracy -> 0.9135

In architecture AutoML was better because it tried a lot of different models, which was quite impossible if we have to do the same task with Hyperdrive because you have to create pipeline for every model.

There was not much in accuracy because of the data set but AutoML really computed some complex models which I was not even thinking to implement.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
Maybe there was some issue with my configuration, I found I have to open the log of each child job and manually click the auth link and authenticate in a page. I'd like to know how to fix this issue.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
Cluster terminated automatically after workshop timedout.
